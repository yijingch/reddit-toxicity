{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OWTe9VS3_b11"
   },
   "source": [
    "## SI 670 Applied Machine Learning, Week 6:  Naive Bayes, Pipeline, High Dimension, Density Estimation. (Due 10/19 11:59pm)\n",
    "\n",
    "For this assignment, each question is worth 20 points, for a total of 60 points. Correct answers and code receive full credit, but partial credit will be awarded if you have the right idea even if your final answers aren't quite right.\n",
    "\n",
    "Submit your completed notebook file AND corresponding **HTML** file to the Canvas site.\n",
    "\n",
    "As a reminder, the notebook code you submit must be your own work. Feel free to discuss general approaches to the homework with classmates: if you end up forming more of a team discussion on multiple questions, please include the names of the people you worked with at the top of your notebook file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put your name here: `Yijing Chen`\n",
    "\n",
    "### Put your uniquename here: `yijingch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 (20 points)\n",
    "\n",
    "Please write the answers as well as your derivation process of the following questions. You can use either LaTeX or python code to represent your answer. For example, if you want to present <$x_1^2$>, in the LaTeX format you should write <(dollar sign) x_1^2 (dollar sign)>; in the python code format you should write <\\`x_1\\*\\*2\\`>. See [here](https://csrgxtu.github.io/2015/03/20/Writing-Mathematic-Fomulars-in-Markdown/) for how to represent more mathmatical symbols in LaTeX format.\n",
    "\n",
    "**Calculate the unnormalized posterior probability of a naive Bayes classifier**\n",
    "\n",
    "Suppose you have a dataset with 2 features $X_1, X_2$ and a binary label $Y$. $X_1$ and $Y$ takes value either 0 or 1. $X_2$ takes one out of the three possible values $0, 1$, or $2$.\n",
    "\n",
    "Based on the dataset, you know \n",
    "\n",
    "$p(Y=0) = 0.05$, \n",
    "\n",
    "$p(X_1=0 | Y=0) = 0.6$, $p(X_1=0 | Y=1) = 0.3$, \n",
    "\n",
    "$p(X_2=0 | Y=0) = 0.9$, $p(X_2=1 | Y=0) = 0.05$, $p(X_2=0 | Y=1) = 0.1$, $p(X_2=1 | Y=1) = 0.3$. \n",
    "\n",
    "Please calculate the unnormalized posterior probability of a naive Bayes classifier: \n",
    "$$\\hat{p}(Y=1 | X) = p(X | Y=1) p(Y=1)$$ and $$\\hat{p}(Y=0 | X) = p(X | Y=0) p(Y=0)$$ of the following data points.\n",
    "\n",
    "#### (a) (10 points) $X_1 = 1, X_2 = 0$\n",
    "\n",
    "#### (b) (10 points) $X_1 = 0, X_2 = 2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "**Known**\n",
    "\n",
    "$p(Y=0) = 0.05$, $p(Y=1) = 0.95$,\n",
    "\n",
    "$p(X_1=0 | Y=0) = 0.6$, $p(X_1=1 | Y=0) = 1-0.6 = 0.4$,\n",
    "\n",
    "$p(X_1=0 | Y=1) = 0.3$, $p(X_1=1 | Y=1) = 1-0.3 = 0.7$, \n",
    "\n",
    "$p(X_2=0 | Y=0) = 0.9$, $p(X_2=1 | Y=0) = 0.05$, $p(X_2=2 | Y=0) = 1-0.9-0.05 = 0.05$, \n",
    "\n",
    "$p(X_2=0 | Y=1) = 0.1$, $p(X_2=1 | Y=1) = 0.3$, $p(X_2=2 | Y=1) = 1-0.1-0.3 = 0.6$,  \n",
    "\n",
    "(a)  \n",
    "\n",
    "$\\hat{p}(Y=1 | X=(1,0)) = 0.95 \\times 0.7 \\times 0.1 = 0.0665 $  \n",
    "\n",
    "$\\hat{p}(Y=0 | X=(1,0)) = 0.05 \\times 0.4 \\times 0.9 = 0.0180 $\n",
    "\n",
    "(b)\n",
    "\n",
    "$\\hat{p}(Y=1 | X=(0,2)) = 0.95 \\times 0.3 \\times 0.6 = 0.1710 $  \n",
    "\n",
    "$\\hat{p}(Y=0 | X=(0,2)) = 0.05 \\times 0.6 \\times 0.05 = 0.0015 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06649999999999999\n",
      "0.018000000000000006\n",
      "0.17099999999999999\n",
      "0.0015\n"
     ]
    }
   ],
   "source": [
    "print(0.95*0.7*0.1)\n",
    "print(0.05*0.4*0.9)\n",
    "print(0.95*0.3*0.6)\n",
    "print(0.05*0.6*0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 (20 points)\n",
    "\n",
    "Build a pipeline that first applies the MinMaxScaler to the data and then use the linearSVC. After that, apply the GridSearchCV by using pipeline as the estimator to search the hyper-parameter `C` of the `LinearSVC` classifier.\n",
    "\n",
    "Return a number `test_score`, where `test_score` is the accuracy score you get from your final model on `(X_test, y_test)`.\n",
    "\n",
    "The grid search range of the parameter is given in `param_grid`.\n",
    "\n",
    "*Hint1: The `GridSearchCV` itself can be viewed as a classifier or a regressor because it implements `.fit` and `.score` functions.*\n",
    "\n",
    "*Hint2: The correct way to combine `GridSearchCV` and `Pipeline` is: you can grid search a pipeline with the scaler and the classifier.*\n",
    "\n",
    "*Hint3: You could refer to the offical document about how to use it: https://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.html*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter (CV score=0.9835567715458277): {'LinerSVC__C': 10}\n",
      "test score = 0.958041958041958\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.958041958041958"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_two():\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.datasets import load_breast_cancer\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    (X_cancer, y_cancer) = load_breast_cancer(return_X_y = True)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer, random_state = 0)\n",
    "\n",
    "    param_grid = [{'LinerSVC__C': [0.1, 1, 10, 100], }]\n",
    "    \n",
    "    components = [(\"scaler\", MinMaxScaler()), (\"LinerSVC\", LinearSVC())]\n",
    "    pipe = Pipeline(components)\n",
    "    \n",
    "    CV_LinearSVC = GridSearchCV(pipe, param_grid)\n",
    "    CV_LinearSVC.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best parameter (CV score={CV_LinearSVC.best_score_}): {CV_LinearSVC.best_params_}\")\n",
    "\n",
    "    test_score = CV_LinearSVC.score(X_test, y_test)\n",
    "    print(f\"test score = {test_score}\")\n",
    "    return test_score\n",
    "\n",
    "answer_two()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 (20 points)\n",
    "In this question, we hope to compare the GradientBoostingRegressor, AdaBoostRegressor, and XGBRegressor. In lab 6, we have given you an example about how to use XGBRegressor. Similiar to that, we hope you to explore how to use GradientBoostingRegressor and AdaBoostRegressor to predict the boston housing price. \n",
    "\n",
    "We would use random_state=0, n_estimators = 50 for three regressor. \n",
    "\n",
    "You are supposed to return three numbers: the mean_squared_error of XGBRegressor, the mean_squared_error of GradientBoostingRegressor, the mean_squared_error of AdaBoostRegressor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])\n",
      "(506, 13)\n",
      "XGBRegressor: test rmse = 4.622928681419338\n",
      "rmse on training set = 0.1329027461011375\n",
      "GradientBoostingRegressor: test rmse = 4.402475712441904\n",
      "rmse on training set = 1.657326198437047\n",
      "AdaBoostRegressor: test rmse = 5.286321698508684\n",
      "rmse on training set = 2.629244638676872\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4.622928681419338, 4.402475712441904, 5.286321698508684)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
    "\n",
    "#import the Boston Housing dataset and store it in a variable called boston\n",
    "boston = load_boston()\n",
    "print(boston.keys())\n",
    "print(boston.data.shape)\n",
    "\n",
    "data = pd.DataFrame(boston.data)\n",
    "data.columns = boston.feature_names\n",
    "\n",
    "#get the y label, i.e., price \n",
    "data['PRICE'] = boston.target\n",
    "\n",
    "#get the X, y data\n",
    "X, y = data.iloc[:,:-1],data.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "def answer_three():   \n",
    "#     xg_reg = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=0, n_estimators=50)\n",
    "    xg_reg = xgb.XGBRegressor(random_state=0, n_estimators=50)\n",
    "    xg_reg.fit(X_train,y_train)\n",
    "    xg_preds = xg_reg.predict(X_test)\n",
    "    xg_rmse = np.sqrt(mean_squared_error(y_test, xg_preds))\n",
    "    print(f\"XGBRegressor: test rmse = {xg_rmse}\")\n",
    "    xg_preds_train = xg_reg.predict(X_train)\n",
    "    xg_rmse_train = np.sqrt(mean_squared_error(y_train, xg_preds_train))\n",
    "    print(f\"rmse on training set = {xg_rmse_train}\")\n",
    "    \n",
    "    gra_reg = GradientBoostingRegressor(random_state=0, n_estimators=50)\n",
    "    gra_reg.fit(X_train, y_train)\n",
    "    gra_preds = gra_reg.predict(X_test)\n",
    "    gra_rmse = np.sqrt(mean_squared_error(y_test, gra_preds))\n",
    "    print(f\"GradientBoostingRegressor: test rmse = {gra_rmse}\")\n",
    "    gra_preds_train = gra_reg.predict(X_train)\n",
    "    gra_rmse_train = np.sqrt(mean_squared_error(y_train, gra_preds_train))\n",
    "    print(f\"rmse on training set = {gra_rmse_train}\")\n",
    "    \n",
    "    ada_reg = AdaBoostRegressor(random_state=0, n_estimators=50)\n",
    "    ada_reg.fit(X_train, y_train)\n",
    "    ada_preds = ada_reg.predict(X_test)\n",
    "    ada_rmse = np.sqrt(mean_squared_error(y_test, ada_preds))\n",
    "    print(f\"AdaBoostRegressor: test rmse = {ada_rmse}\")\n",
    "    ada_preds_train = ada_reg.predict(X_train)\n",
    "    ada_rmse_train = np.sqrt(mean_squared_error(y_train, ada_preds_train))\n",
    "    print(f\"rmse on training set = {ada_rmse_train}\")\n",
    "    \n",
    "    return xg_rmse, gra_rmse, ada_rmse\n",
    "\n",
    "answer_three()  # overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "si670f19_lab_2_ans.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
